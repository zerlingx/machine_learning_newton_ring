{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit"
  },
  "interpreter": {
   "hash": "2b9bd1328ed778785c413a0e420e20797c7846037ddcfd978709d10a846a2f45"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# machine_learning_newton_ring\n",
    ">牛顿环中心坐标估算主程序"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 00 基本依赖项加载"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载依赖项\n",
    "from cv2 import data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from numpy import random\n",
    "from numpy.lib.type_check import imag\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "\n",
    "from mindspore.common import dtype as mstype\n",
    "from mindspore import context\n",
    "\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "from mindspore import nn\n",
    "from mindspore.train import Model\n",
    "from mindspore.train.callback import Callback\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore import Tensor\n",
    "from mindspore.train.serialization import export\n",
    "from mindspore.ops import operations as P\n",
    "\n",
    "# 设置MindSpore的执行模式和设备\n",
    "#context.GRAPH_MODE指向静态图模型，即在运行之前会把全部图建立编译完毕。设备指定为Ascend处理器。\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\")"
   ]
  },
  {
   "source": [
    "## 01 数据生成与加载"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义项\n",
    "\n",
    "# mindspore自定义数据集生成器\n",
    "class DatasetGenerator:\n",
    "    def __init__(self):\n",
    "        img = np.array(Image.open(\"../data/img/train_img_1.jpg\"))\n",
    "        path = \"../data/img/train_img_\"\n",
    "        for img_i in range(3049):\n",
    "            img_nxt = np.array(Image.open(path+str(img_i+2)+\".jpg\"))\n",
    "            if img_i == 0:\n",
    "                img = np.stack((img,img_nxt))\n",
    "            else:\n",
    "                img_nxt = img_nxt[np.newaxis, :]\n",
    "                img = np.vstack((img, img_nxt))\n",
    "        self.data = img\n",
    "        ct = np.load(\"../data/center/center.npy\")\n",
    "        self.label = ct\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成、加载./data中的数据\n",
    "dataset_generator = DatasetGenerator()\n",
    "dataset = ds.GeneratorDataset(dataset_generator, [\"data\", \"label\"], shuffle=False)\n",
    "\n",
    "print(\"\\n01 ---dataset generate complete---\\n\")\n",
    "\n",
    "# 全部打印\n",
    "# for data in dataset.create_dict_iterator():\n",
    "#     print('{}'.format(data[\"data\"]), '{}'.format(data[\"label\"]))\n",
    "\n",
    "# 创建数据集的迭代器p_data\n",
    "p_data = dataset.create_dict_iterator()"
   ]
  },
  {
   "source": [
    "## 02 数据处理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "\n",
    "# 通道格式设置\n",
    "hwc2chw_op = CV.HWC2CHW()\n",
    "dataset = dataset.map(input_columns=\"data\", operations=hwc2chw_op)\n",
    "# 变成float32\n",
    "type_cast_op = C.TypeCast(mstype.float32)\n",
    "dataset = dataset.map(input_columns=\"data\", operations=type_cast_op)\n",
    "# 归一化\n",
    "rescale = 1.0 / 255.5\n",
    "shift = 0\n",
    "rescale_op = CV.Rescale(rescale, shift)\n",
    "dataset = dataset.map(input_columns=\"data\", operations=rescale_op)\n",
    "#打乱顺序\n",
    "dataset = dataset.shuffle(buffer_size=3050)\n",
    "#划分数据集\n",
    "(ds_train,ds_test) = dataset.split([0.8,0.2])\n",
    "#设置batch_size\n",
    "batch_size = 50\n",
    "ds_train = ds_train.batch(batch_size, drop_remainder=True)\n",
    "ds_test  = ds_test.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"测试集大小 \",ds_train.get_dataset_size()*batch_size,\"组\")\n",
    "print(\"训练集大小 \",ds_test.get_dataset_size()*batch_size,\"组\")\n",
    "# 选三组数据和标签打印，检查是否加载和处理正确\n",
    "test_p_data = dataset.create_dict_iterator(output_numpy=True)\n",
    "for i in range(3):\n",
    "    test_data = next(test_p_data)\n",
    "    num = random.randint(1,3050)\n",
    "    # print('{}'.format(test_data[\"data\"].shape), '{}'.format(test_data[\"label\"])) #打印数据\n",
    "    img = test_data[\"data\"]\n",
    "    ct = test_data[\"label\"]\n",
    "    print(img.shape, ct)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.colorbar()\n",
    "    plt.scatter(int(test_data[\"label\"][0]), int(test_data[\"label\"][1]), color='r',s=20)\n",
    "    plt.show()  #打印图像\n",
    "print(\"\\n02 ---dataset proceed complete---\\n\")"
   ]
  },
  {
   "source": [
    "## 03 模型构建"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class VGG(nn.Cell): \n",
    "    # 神经网络的各层需要预先在__init__方法中定义，然后通过定义construct方法来完成神经网络的前向构造\n",
    "    def __init__(self, num_class=2,channel=1,dropout_ratio=0.7,trun_sigma=0.01):\n",
    "        super(VGG, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.channel = channel\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        #设置卷积层\n",
    "        self.conv11 = nn.Conv2d(self.channel, 64, #224*224*64\n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        self.conv12 = nn.Conv2d(64, 64,            \n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        self.conv21 = nn.Conv2d(64, 128, #112*112*128\n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        self.conv22 = nn.Conv2d(128, 128,\n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        self.conv31 = nn.Conv2d(128, 256, #56*56*256\n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        self.conv32 = nn.Conv2d(256, 256,\n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        self.conv33 = nn.Conv2d(256, 256,\n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        self.conv41 = nn.Conv2d(256, 512, #28*28*512\n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        self.conv42 = nn.Conv2d(512, 512,\n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        self.conv43 = nn.Conv2d(512, 512,\n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        self.conv51 = nn.Conv2d(512, 512, #14*14*512\n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        self.conv52 = nn.Conv2d(512, 512,\n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        self.conv53 = nn.Conv2d(512, 512,\n",
    "                               kernel_size=3, stride=1, \n",
    "                               has_bias=True, pad_mode=\"same\",\n",
    "                               weight_init='xavier_uniform', bias_init='zeros')\n",
    "        #设置最大池化层\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.max_pool_mid = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        #设置ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(self.dropout_ratio)\n",
    "        \n",
    "        self.fc1 = nn.Dense(7*7*512,4096,weight_init=TruncatedNormal(sigma=trun_sigma),bias_init = 0.1)\n",
    "        self.fc2 = nn.Dense(4096, 4096, weight_init=TruncatedNormal(sigma=trun_sigma), bias_init=0.1)\n",
    "        self.fc3 = nn.Dense(4096, 1024, weight_init=TruncatedNormal(sigma=trun_sigma), bias_init=0.1)\n",
    "        self.fc4 = nn.Dense(1024, self.num_class, weight_init=TruncatedNormal(sigma=trun_sigma), bias_init=0.1)\n",
    "    #构建模型    \n",
    "    def construct(self, x):\n",
    "        x = self.conv11(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv12(x)          \n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv21(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv22(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv31(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv32(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv33(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv41(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv42(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv43(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv51(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv52(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv53(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "# 构建完成后，可以使用print将神经网络中的各层参数全部打印出来\n",
    "print(VGG())\n",
    "print(\"\\n03-1 ---model set complete---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数（Loss）和优化器（Optimizer）\n",
    "\n",
    "net=VGG(num_class=2, channel=1, dropout_ratio=0.5,trun_sigma=0.01)\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "#opt\n",
    "fc_weight_params = list(filter(lambda x: 'fc' in x.name and 'weight' in x.name, net.trainable_params()))\n",
    "other_params=list(filter(lambda x: 'fc' not in x.name or 'weight' not in x.name, net.trainable_params()))\n",
    "group_params = [{'params': fc_weight_params, 'weight_decay': 0.01},\n",
    "                {'params': other_params},\n",
    "                {'order_params': net.trainable_params()}]\n",
    "net_opt = nn.Adam(group_params, learning_rate=0.0001, weight_decay=0.0)\n",
    "print(\"\\n03-2 ---loss and optimizer set complete---\\n\")"
   ]
  },
  {
   "source": [
    "## 04 开始训练"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义回调函数收集模型的损失值和精度值，为之后对训练过程进行可视化显示做准备\n",
    "class StepLossAccInfo(Callback):\n",
    "    def __init__(self, model, eval_dataset, steps_loss, steps_eval):\n",
    "        self.model = model\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.steps_loss = steps_loss\n",
    "        self.steps_eval = steps_eval\n",
    "\n",
    "    def step_end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        cur_epoch = cb_params.cur_epoch_num\n",
    "        cur_step = (cur_epoch-1)*2400 + cb_params.cur_step_num\n",
    "        self.steps_loss[\"loss_value\"].append(str(cb_params.net_outputs))\n",
    "        self.steps_loss[\"step\"].append(str(cur_step))\n",
    "        if cur_step % 48 == 0:\n",
    "            acc = self.model.eval(self.eval_dataset, dataset_sink_mode=True)\n",
    "            self.steps_eval[\"step\"].append(cur_step)\n",
    "            self.steps_eval[\"acc\"].append(acc[\"acc\"])\n",
    "            \n",
    "model = Model(net, loss_fn=net_loss, optimizer=net_opt, metrics={'acc', 'loss'})\n",
    "\n",
    "time_cb = TimeMonitor()    # 监控每个epoch训练的时间\n",
    "loss_cb = LossMonitor(per_print_times=ds_train.get_dataset_size())  #LossMonitor用于打印损失函数\n",
    "\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=960,keep_checkpoint_max=10)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"VGG\", directory=\"./output_model/checkpoint\", config=config_ck)\n",
    "\n",
    "steps_loss = {\"step\": [], \"loss_value\": []}\n",
    "steps_eval = {\"step\": [], \"acc\": []}\n",
    "step_loss_acc_info = StepLossAccInfo(model, ds_test, steps_loss, steps_eval)\n",
    "\n",
    "print(\"============== Starting Training ==============\")\n",
    "epoch_size = 500 # 训练次数\n",
    "model.train(epoch_size, ds_train, callbacks=[loss_cb, ckpoint_cb, time_cb, step_loss_acc_info], dataset_sink_mode=True)\n",
    "print('Checkpoints after resuming training:')\n",
    "print('\\n'.join(sorted([x for x in os.listdir(\"./output_model/checkpoint\") if x.startswith('VGG')])))\n",
    "print(\"\\n04 ---training complete---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}